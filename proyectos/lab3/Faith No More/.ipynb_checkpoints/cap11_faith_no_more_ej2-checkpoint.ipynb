{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cede7b14",
   "metadata": {},
   "source": [
    "# Extracción de features a partir de variables de texto\n",
    "\n",
    "Podemos extraer mucha información de las variables de texto para utilizarlas como características predictivas en modelos de machine learning.\n",
    "Las técnicas que se describen en este capítulo pertenecen al campo del *Procesamiento del Lenguaje Natural (NLP)*, una rama de la lingüística y la informática que se ocupa de la interacón entre los ordenadores y el lenguaje humano; en otras palabras, cómo programar computadoras para que comprendan el lenguaje humano.\n",
    "El NLP incluye muchas técnicas para entender la sintaxis, la semántica y el discurso del texto.\n",
    "Hacer justicia a este campo requeriría un libro completo, pero aquí nos centraremos en las técnicas más útiles para extraer rápidamente características de textos cortos, con el fin de complementar nuestros modelos predictivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ec3d4-bf51-48ac-bfe4-e0dd244f663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuración Inicial ##\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- CONFIGURACIÓN DE ENTORNO Y RUTAS ---\n",
    "# Usamos pathlib para ser consistentes con el notebook 'ej1'\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "NLTK_DATA_DIR = PROJECT_ROOT / \"nltk_data\"\n",
    "\n",
    "# Crear directorio local si no existe\n",
    "NLTK_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Añadir esta ruta a la lista de búsqueda de NLTK\n",
    "# Esto asegura que Python busque aquí antes de intentar descargar de nuevo\n",
    "nltk.data.path.append(str(NLTK_DATA_DIR))\n",
    "\n",
    "# Descargar recursos necesarios en la carpeta local (silencioso)\n",
    "print(f\"Verificando recursos NLTK en: {NLTK_DATA_DIR} ...\")\n",
    "try:\n",
    "    nltk.download('punkt_tab', download_dir=str(NLTK_DATA_DIR), quiet=True)\n",
    "    nltk.download('stopwords', download_dir=str(NLTK_DATA_DIR), quiet=True)\n",
    "    print(\"✓ Recursos configurados correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error descargando recursos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f391727-539b-4b54-ae68-a4814468be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONSTANTES GLOBALES (Best Practices) ---\n",
    "\n",
    "# 1. Compilar Regex: Lo hacemos una sola vez para mejorar el rendimiento\n",
    "# Esta expresión mantiene solo letras (a-z) y espacios.\n",
    "REGEX_SOLO_LETRAS = re.compile(r'[^a-zA-Z\\s]')\n",
    "\n",
    "# 2. Inicializar Stemmer\n",
    "STEMMER = SnowballStemmer('english')\n",
    "\n",
    "# 3. Cargar Stop Words como un 'set' (búsqueda instantánea O(1))\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Total de stop words cargadas: {len(STOP_WORDS)}\")\n",
    "print(f\"Ejemplos: {list(STOP_WORDS)[:10]}\")\n",
    "\n",
    "# --- FUNCIÓN PRINCIPAL DE LIMPIEZA ---\n",
    "def limpiar_y_stemming(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesa un texto para NLP.\n",
    "    \n",
    "    Pasos:\n",
    "    1. Elimina caracteres no alfabéticos.\n",
    "    2. Convierte a minúsculas.\n",
    "    3. Tokeniza (divide en palabras de forma inteligente).\n",
    "    4. Elimina stopwords.\n",
    "    5. Aplica stemming.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): El texto original (review).\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto procesado y unido nuevamente.\n",
    "    \"\"\"\n",
    "    # Validación: si no es texto (ej. NaN), retornar vacío\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Eliminar puntuación y números usando regex pre-compilado\n",
    "    texto_limpio = REGEX_SOLO_LETRAS.sub('', texto)\n",
    "\n",
    "    # 2. Convertir a minúsculas\n",
    "    texto_minusculas = texto_limpio.lower()\n",
    "    \n",
    "    # 3. Tokenizar (word_tokenize maneja mejor la puntuación que .split())\n",
    "    tokens = word_tokenize(texto_minusculas)\n",
    "    \n",
    "    # 4. Eliminar stop words y 5. Aplicar stemming\n",
    "    # Usamos list comprehension que es más eficiente que el bucle for tradicional\n",
    "    tokens_procesados = [\n",
    "        STEMMER.stem(token) \n",
    "        for token in tokens \n",
    "        if token not in STOP_WORDS and len(token) > 1\n",
    "    ]\n",
    "    \n",
    "    # Unir las palabras de nuevo en un solo string\n",
    "    return ' '.join(tokens_procesados)csv_file = \"IMDB Dataset.csv\"\n",
    "\n",
    "if Path(csv_file).exists():\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas.\")\n",
    "else:\n",
    "    print(f\"⚠️ ADVERTENCIA: No se encontró '{csv_file}'. Generando datos de prueba.\")\n",
    "    # Datos dummy para que el notebook sea reproducible incluso sin el CSV\n",
    "    df = pd.DataFrame({\n",
    "        'review': [\n",
    "            \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked.\",\n",
    "            \"A wonderful little production. <br /><br />The filming technique is very unassuming.\",\n",
    "            \"Phil the Alien is one of those quirky films where the humour is based around the oddness.\",\n",
    "            \"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet.\"\n",
    "        ],\n",
    "        'sentiment': ['positive', 'positive', 'positive', 'negative']\n",
    "    })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6653ebf-ca86-4c97-9abf-fe66e2f887f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"IMDB Dataset.csv\"\n",
    "\n",
    "if Path(csv_file).exists():\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas.\")\n",
    "else:\n",
    "    print(f\"⚠️ ADVERTENCIA: No se encontró '{csv_file}'. Generando datos de prueba.\")\n",
    "    # Datos dummy para que el notebook sea reproducible incluso sin el CSV\n",
    "    df = pd.DataFrame({\n",
    "        'review': [\n",
    "            \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked.\",\n",
    "            \"A wonderful little production. <br /><br />The filming technique is very unassuming.\",\n",
    "            \"Phil the Alien is one of those quirky films where the humour is based around the oddness.\",\n",
    "            \"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet.\"\n",
    "        ],\n",
    "        'sentiment': ['positive', 'positive', 'positive', 'negative']\n",
    "    })\n",
    "\n",
    "df.head()# Demostración del proceso con un solo ejemplo\n",
    "# Usamos un índice seguro (el primero disponible)\n",
    "indice_ejemplo = 2 if len(df) > 2 else 0\n",
    "texto_original = df['review'].iloc[indice_ejemplo]\n",
    "\n",
    "print(\"--- 1. Texto Original ---\")\n",
    "print(texto_original)\n",
    "\n",
    "# Aplicamos la función que definimos arriba\n",
    "texto_final = limpiar_y_stemming(texto_original)\n",
    "\n",
    "print(\"\\n--- 2. Texto Procesado (Limpieza + Stemming) ---\")\n",
    "print(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dacb07-932c-48ee-8e05-90fef22168c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la función a toda la columna 'review'\n",
    "print(\"Iniciando preprocesamiento masivo...\")\n",
    "\n",
    "# .apply() es eficiente, pero para datasets gigantes (>1GB) consideraríamos librerías como swifter\n",
    "df['review_limpia'] = df['review'].apply(limpiar_y_stemming)\n",
    "\n",
    "print(\"¡Preprocesamiento completado!\")\n",
    "\n",
    "# Mostramos resultado final comparativo\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "display(df[['review', 'review_limpia']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
