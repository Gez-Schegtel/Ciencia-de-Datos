{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbdfd07d-3914-4aa2-bac9-78d72c222d1b",
   "metadata": {},
   "source": [
    "# Resumen del Capítulo: Ingeniería de Características a partir de Texto\n",
    "\n",
    "## Introducción al Capítulo\n",
    "\n",
    "En muchos conjuntos de datos, la información puede provenir de campos de texto libre,\n",
    "como descripciones de incidentes o reseñas de clientes. A diferencia de los datos tabulares,\n",
    "el texto varía en longitud, contenido y estilo de escritura. El objetivo de este capítulo\n",
    "es mostrar cómo transformar esta información textual en características predictivas numéricas\n",
    "que pueden ser utilizadas en modelos de machine learning.\n",
    "\n",
    "Las técnicas cubiertas pertenecen al campo del **Procesamiento del Lenguaje Natural (NLP)**,\n",
    "que se ocupa de programar computadoras para comprender el lenguaje humano. En concreto,\n",
    "el capítulo se enfoca en métodos para extraer rápidamente características de textos cortos,\n",
    "capturando su complejidad a través de parámetros estadísticos (como la longitud de las palabras,\n",
    "el número de palabras únicas y el conteo de oraciones).\n",
    "\n",
    "## Librerías y Requisitos Técnicos\n",
    "\n",
    "| Librería           | Propósito                                                                                     |\n",
    "|--------------------|-----------------------------------------------------------------------------------------------|\n",
    "| **pandas**         | Manipulación de datos y funciones vectorizadas de procesamiento de strings (str).            |\n",
    "| **scikit-learn**   | Carga de conjuntos de datos (e.g., 20 Newsgroup) y transformers clave.                      |\n",
    "| **NLTK**           | Herramienta integral de Python para NLP, esencial para la tokenización y stemming.          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12cf937a-f154-4b68-b5d5-dabe5f584065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando recursos de NLTK...\n",
      "✓ Recursos de NLTK descargados\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS - Librerías y Módulos Necesarios\n",
    "# ============================================================================\n",
    "\n",
    "# --- Sistema Operativo y Rutas ---\n",
    "import os  # Operaciones del sistema operativo (no usado directamente, pero disponible)\n",
    "from pathlib import (\n",
    "    Path,\n",
    ")\n",
    "\n",
    "# --- Manipulación de Datos ---\n",
    "import pandas as pd  # Análisis y manipulación de datos en DataFrames\n",
    "# Usado para: crear tablas, aplicar operaciones vectorizadas en texto\n",
    "\n",
    "# --- Natural Language Toolkit (NLTK) - Core ---\n",
    "import nltk  # Librería principal de NLP para Python\n",
    "# Usado para: descargar recursos y configurar rutas de datos\n",
    "\n",
    "# --- NLTK - Tokenización ---\n",
    "from nltk.tokenize import sent_tokenize  # Divide texto en oraciones individuales\n",
    "# Usado en: Receta 2 para contar número de oraciones\n",
    "\n",
    "# --- NLTK - Stop Words ---\n",
    "from nltk.corpus import (\n",
    "    stopwords,\n",
    ")  # Acceso a listas de palabras comunes sin valor semántico\n",
    "# Usado en: Receta 5 para filtrar palabras como 'the', 'a', 'is'\n",
    "\n",
    "# --- NLTK - Stemming ---\n",
    "from nltk.stem.snowball import SnowballStemmer  # Reduce palabras a su raíz/base\n",
    "# Usado en: Receta 5 para convertir 'running', 'runs' -> 'run'\n",
    "\n",
    "# --- Scikit-learn - Datasets ---\n",
    "from sklearn.datasets import (\n",
    "    fetch_20newsgroups,\n",
    ")  # Descarga dataset de grupos de noticias\n",
    "# Usado para: obtener textos de ejemplo para análisis\n",
    "\n",
    "# --- Scikit-learn - Vectorización de Texto ---\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    ")  # Convierte texto a matriz Bag-of-Words\n",
    "# Usado en: Receta 3 para contar frecuencia de palabras\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer,\n",
    ")  # Convierte texto a matriz TF-IDF\n",
    "# Usado en: Receta 4 para ponderar importancia de palabras\n",
    "\n",
    "# Configuración de directorios del proyecto\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "NLTK_DATA_DIR = PROJECT_ROOT / \"nltk_data\"\n",
    "SKLEARN_DATA_DIR = PROJECT_ROOT / \"sklearn_data\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "NLTK_DATA_DIR.mkdir(exist_ok=True)\n",
    "SKLEARN_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configurar NLTK para usar el directorio del proyecto\n",
    "nltk.data.path.insert(0, str(NLTK_DATA_DIR))\n",
    "\n",
    "# Descargar recursos de NLTK en el directorio del proyecto\n",
    "print(\"Descargando recursos de NLTK...\")\n",
    "nltk.download(\"punkt\", download_dir=str(NLTK_DATA_DIR), quiet=True)\n",
    "nltk.download(\"stopwords\", download_dir=str(NLTK_DATA_DIR), quiet=True)\n",
    "nltk.download(\"punkt_tab\", download_dir=str(NLTK_DATA_DIR), quiet=True)\n",
    "print(\"✓ Recursos de NLTK descargados\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c5a141-fd06-4bf7-945e-597d4d44fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para cargar datos en el directorio del proyecto\n",
    "def load_newsgroups_data(subset=\"train\"):\n",
    "    \"\"\"\n",
    "    Carga el dataset 20 Newsgroups en el directorio del proyecto.\n",
    "\n",
    "    Parameters:\n",
    "        subset (str): 'train' o 'test'\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con columna 'text'\n",
    "    \"\"\"\n",
    "    print(f\"Cargando dataset 20 Newsgroups ({subset})...\")\n",
    "    data = fetch_20newsgroups(\n",
    "        subset=subset,\n",
    "        data_home=str(SKLEARN_DATA_DIR),\n",
    "        remove=(\"headers\", \"footers\", \"quotes\"),  # Limpieza inicial\n",
    "    )\n",
    "    df = pd.DataFrame(data.data, columns=[\"text\"])\n",
    "    print(f\"✓ Dataset cargado: {len(df)} documentos\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2ab91-62f2-4532-8fa3-98051dc56f2e",
   "metadata": {},
   "source": [
    "### Verificación del dataset original y de los archivos descargados.\n",
    "\n",
    "Antes de comenzar con las recetas, vemos un ejemplo del dataset descargado sin procesar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ece05a6-52db-4fdf-9a8d-11444208f6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset 20 Newsgroups (train)...\n",
      "✓ Dataset cargado: 11314 documentos\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  I was wondering if anyone out there could enli...\n",
       "1  A fair number of brave souls who upgraded thei...\n",
       "2  well folks, my mac plus finally gave up the gh...\n",
       "3  \\nDo you have Weitek's address/phone number?  ...\n",
       "4  From article <C5owCB.n3p@world.std.com>, by to..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_newsgroups_data(subset=\"train\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769723a3-0cc3-4379-ab6d-aac486bb5591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VERIFICACIÓN DE ARCHIVOS DEL PROYECTO\n",
      "======================================================================\n",
      "\n",
      "Directorio del proyecto: /home/juani/Documentos/Facultad/Ciencia de Datos/proyectos/lab3/Faith No More\n",
      "\n",
      "Archivos NLTK: /home/juani/Documentos/Facultad/Ciencia de Datos/proyectos/lab3/Faith No More/nltk_data\n",
      " ✓ 179 archivos encontrados\n",
      "\n",
      "Archivos Scikit-learn: /home/juani/Documentos/Facultad/Ciencia de Datos/proyectos/lab3/Faith No More/sklearn_data\n",
      " ✓ 1 archivos encontrados\n",
      "\n",
      " ✓ Todos los archivos están en el directorio del proyecto\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICACIÓN DE ARCHIVOS DEL PROYECTO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDirectorio del proyecto: {PROJECT_ROOT}\")\n",
    "print(f\"\\nArchivos NLTK: {NLTK_DATA_DIR}\")\n",
    "if NLTK_DATA_DIR.exists():\n",
    "    nltk_files = list(NLTK_DATA_DIR.rglob(\"*\"))\n",
    "    print(f\" ✓ {len(nltk_files)} archivos encontrados\")\n",
    "\n",
    "print(f\"\\nArchivos Scikit-learn: {SKLEARN_DATA_DIR}\")\n",
    "if SKLEARN_DATA_DIR.exists():\n",
    "    sklearn_files = list(SKLEARN_DATA_DIR.rglob(\"*\"))\n",
    "    print(f\" ✓ {len(sklearn_files)} archivos encontrados\")\n",
    "\n",
    "print(\"\\n ✓ Todos los archivos están en el directorio del proyecto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744b9a5-1999-4bbc-9e11-e080d77539a5",
   "metadata": {},
   "source": [
    "# Recetas Clave del Capítulo\n",
    "\n",
    "El capítulo se estructura alrededor de cinco recetas principales, que transforman \n",
    "el texto sin procesar en datos estructurados y numéricos.\n",
    "\n",
    "## **Receta 1:** Conteo de Caracteres, Palabras y Vocabulario\n",
    "\n",
    "Esta receta se centra en medir la complejidad del texto a través de estadísticas básicas. \n",
    "Las descripciones más largas y ricas en vocabulario único suelen contener más información.\n",
    "\n",
    "Características extraídas (usando pandas):\n",
    "1. **Número total de caracteres:** Incluye letras, números, símbolos y espacios.\n",
    "2. **Número total de palabras**.\n",
    "3. **Número total de palabras únicas (vocabulario)**.\n",
    "4. **Diversidad léxica:** Cociente entre el número total de palabras y el número de palabras únicas.\n",
    "5. **Longitud promedio de la palabra:** Cociente entre el número de caracteres y el número de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132b68c6-8585-4b60-9284-683e1cd24154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RECETA 1: Conteo de Caracteres, Palabras y Vocabulario\n",
      "======================================================================\n",
      "Cargando dataset 20 Newsgroups (train)...\n",
      "✓ Dataset cargado: 11314 documentos\n",
      "\n",
      "\n",
      "Estadísticas básicas del texto:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_char</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_vocab</th>\n",
       "      <th>lexical_div</th>\n",
       "      <th>ave_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11314.000000</td>\n",
       "      <td>11314.000000</td>\n",
       "      <td>11314.000000</td>\n",
       "      <td>11314.000000</td>\n",
       "      <td>11314.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1216.228566</td>\n",
       "      <td>185.827382</td>\n",
       "      <td>106.218844</td>\n",
       "      <td>1.322594</td>\n",
       "      <td>5.912501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4038.305818</td>\n",
       "      <td>523.971647</td>\n",
       "      <td>183.974420</td>\n",
       "      <td>0.395230</td>\n",
       "      <td>2.615407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>235.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.131579</td>\n",
       "      <td>5.438596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>490.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.280702</td>\n",
       "      <td>5.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>982.750000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>1.475588</td>\n",
       "      <td>6.176785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>74878.000000</td>\n",
       "      <td>11765.000000</td>\n",
       "      <td>3480.000000</td>\n",
       "      <td>6.438086</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           num_char     num_words     num_vocab   lexical_div  ave_word_length\n",
       "count  11314.000000  11314.000000  11314.000000  11314.000000     11314.000000\n",
       "mean    1216.228566    185.827382    106.218844      1.322594         5.912501\n",
       "std     4038.305818    523.971647    183.974420      0.395230         2.615407\n",
       "min        0.000000      0.000000      0.000000      0.000000         0.000000\n",
       "25%      235.000000     40.000000     35.000000      1.131579         5.438596\n",
       "50%      490.000000     83.000000     65.000000      1.280702         5.761905\n",
       "75%      982.750000    167.000000    114.000000      1.475588         6.176785\n",
       "max    74878.000000  11765.000000   3480.000000      6.438086        78.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RECETA 1: Conteo de Caracteres, Palabras y Vocabulario\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Cargar datos\n",
    "df = load_newsgroups_data(subset=\"train\")\n",
    "\n",
    "# Conteo de caracteres (después de strip para eliminar espacios en blanco)\n",
    "df[\"num_char\"] = df[\"text\"].str.strip().str.len()\n",
    "\n",
    "# Conteo de palabras (split() divide el texto en espacios en blanco)\n",
    "df[\"num_words\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "# Conteo de vocabulario (palabras únicas)\n",
    "# Usar lower() para evitar que 'Palabra' y 'palabra' sean tratadas como diferentes\n",
    "df[\"num_vocab\"] = df[\"text\"].str.lower().str.split().apply(lambda x: len(set(x)))\n",
    "\n",
    "# Diversidad Léxica (evitar división por cero)\n",
    "df[\"lexical_div\"] = df[\"num_words\"] / df[\"num_vocab\"].replace(0, 1)\n",
    "\n",
    "# Longitud Promedio de Palabras (evitar división por cero)\n",
    "df[\"ave_word_length\"] = df[\"num_char\"] / df[\"num_words\"].replace(0, 1)\n",
    "\n",
    "print(\"\\nEstadísticas básicas del texto:\")\n",
    "df[[\"num_char\", \"num_words\", \"num_vocab\", \"lexical_div\", \"ave_word_length\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da9e55f-3a58-4779-9862-6930ceffa19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_char</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>475</td>\n",
       "      <td>91</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>530</td>\n",
       "      <td>90</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>1659</td>\n",
       "      <td>307</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n",
       "      <td>93</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>448</td>\n",
       "      <td>72</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  num_char  num_words  \\\n",
       "0  I was wondering if anyone out there could enli...       475         91   \n",
       "1  A fair number of brave souls who upgraded thei...       530         90   \n",
       "2  well folks, my mac plus finally gave up the gh...      1659        307   \n",
       "3  \\nDo you have Weitek's address/phone number?  ...        93         15   \n",
       "4  From article <C5owCB.n3p@world.std.com>, by to...       448         72   \n",
       "\n",
       "   num_vocab  \n",
       "0         67  \n",
       "1         74  \n",
       "2        195  \n",
       "3         15  \n",
       "4         61  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nPrimeras 5 filas:\")\n",
    "df[[\"text\", \"num_char\", \"num_words\", \"num_vocab\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c8302-7766-44da-b171-0dcddaa2d616",
   "metadata": {},
   "source": [
    "## **Receta 2:** Estimación de la Complejidad por Conteo de Oraciones\n",
    "\n",
    "Capturar el número de oraciones ofrece información sobre la cantidad de contenido en el texto, \n",
    "ya que las descripciones con múltiples oraciones tienden a ser más informativas. \n",
    "Este proceso se denomina **tokenización de oraciones**.\n",
    "\n",
    "**Nota importante:** La tokenización de oraciones se basa en la puntuación y la capitalización. \n",
    "Si planea contar oraciones, este paso debe realizarse antes de cualquier eliminación de \n",
    "puntuación o cambio de caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe1da142-b609-4877-809e-fe994c10183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RECETA 2: Conteo de Oraciones\n",
      "======================================================================\n",
      "Cargando dataset 20 Newsgroups (train)...\n",
      "✓ Dataset cargado: 11314 documentos\n",
      "\n",
      "\n",
      "Estadísticas de número de oraciones:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11314.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.352130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>31.888797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>921.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           num_sent\n",
       "count  11314.000000\n",
       "mean      11.352130\n",
       "std       31.888797\n",
       "min        0.000000\n",
       "25%        3.000000\n",
       "50%        6.000000\n",
       "75%       10.000000\n",
       "max      921.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECETA 2: Conteo de Oraciones\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Recargar datos limpios (sin headers/footers/quotes)\n",
    "df_sentences = load_newsgroups_data(subset=\"train\")\n",
    "\n",
    "# Función robusta para contar oraciones\n",
    "def count_sentences(text):\n",
    "    \"\"\"Cuenta oraciones manejando textos vacíos.\"\"\"\n",
    "    if pd.isna(text) or not text.strip():\n",
    "        return 0\n",
    "    try:\n",
    "        return len(sent_tokenize(text))\n",
    "    except Exception as e:\n",
    "        print(f\"Error al tokenizar: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Crear característica de número de oraciones\n",
    "df_sentences[\"num_sent\"] = df_sentences[\"text\"].apply(count_sentences)\n",
    "\n",
    "print(\"\\nEstadísticas de número de oraciones:\")\n",
    "df_sentences[[\"num_sent\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e7a91b-c18d-4624-82ec-b56f7845b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  num_sent\n",
       "0  I was wondering if anyone out there could enli...         7\n",
       "1  A fair number of brave souls who upgraded thei...         5\n",
       "2  well folks, my mac plus finally gave up the gh...         8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEjemplos:\")\n",
    "df_sentences[[\"text\", \"num_sent\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be9be7-705b-4296-aa60-b4177ab6d833",
   "metadata": {},
   "source": [
    "## **Receta 3:** Creación de Características con Bag-of-Words y N-grams\n",
    "\n",
    "El **Bag-of-Words (BoW)** es una representación simplificada donde cada palabra única \n",
    "se convierte en una variable, y su valor representa la frecuencia con la que aparece en el texto. \n",
    "El BoW captura la multiplicidad de palabras, pero no su orden o gramática.\n",
    "\n",
    "Para capturar algo de sintaxis, se usan N-grams, que son secuencias contiguas de n ítems \n",
    "(por ejemplo, 2-grams: \"Dogs like\", \"like cats\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e23d7c-7210-4d1d-8eb8-820c8477e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RECETA 3: Bag-of-Words\n",
      "======================================================================\n",
      "Cargando dataset 20 Newsgroups (train)...\n",
      "✓ Dataset cargado: 11314 documentos\n",
      "\n",
      "\n",
      "Dimensiones de la matriz BoW: 11314 filas x 88 columnas\n",
      "\n",
      "Número de features (palabras únicas): 88\n",
      "\n",
      "Primeras 5 palabras más frecuentes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>3964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don</th>\n",
       "      <td>3885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>3752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>3487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Frecuencia\n",
       "people        4103\n",
       "like          3964\n",
       "don           3885\n",
       "just          3752\n",
       "know          3487"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECETA 3: Bag-of-Words\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Recargar datos\n",
    "df_bow = load_newsgroups_data(subset=\"train\")\n",
    "\n",
    "# Limpieza preliminar: Eliminar puntuación y números\n",
    "# Reemplazar con espacio para evitar unir palabras\n",
    "df_bow[\"text_clean\"] = (\n",
    "    df_bow[\"text\"]\n",
    "    .str.replace(r\"[^\\w\\s]\", \" \", regex=True)  # Puntuación -> espacio\n",
    "    .str.replace(r\"\\d+\", \" \", regex=True)  # Números -> espacio\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)  # Múltiples espacios -> uno solo\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Configuración de CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 1),  # Solo unigramas (palabras simples)\n",
    "    min_df=0.05,  # Frecuencia mínima del 5%\n",
    ")\n",
    "\n",
    "# Ajuste y transformación\n",
    "X = vectorizer.fit_transform(df_bow[\"text_clean\"])\n",
    "\n",
    "# Captura del BoW en un DataFrame\n",
    "bagofwords = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\n",
    "    f\"\\nDimensiones de la matriz BoW: {bagofwords.shape[0]} filas x {bagofwords.shape[1]} columnas\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nNúmero de features (palabras únicas): {len(vectorizer.get_feature_names_out())}\"\n",
    ")\n",
    "\n",
    "print(\"\\nPrimeras 5 palabras más frecuentes:\")\n",
    "top_words = bagofwords.sum().sort_values(ascending=False).head()\n",
    "\n",
    "# Convertimos la serie a un DataFrame con un nombre de columna\n",
    "top_words_df = top_words.to_frame(name='Frecuencia')\n",
    "\n",
    "top_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "044b3538-10f7-4b5d-ae74-76d0e696909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 3 filas del BoW:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>actually</th>\n",
       "      <th>available</th>\n",
       "      <th>believe</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>bit</th>\n",
       "      <th>called</th>\n",
       "      <th>case</th>\n",
       "      <th>com</th>\n",
       "      <th>...</th>\n",
       "      <th>used</th>\n",
       "      <th>using</th>\n",
       "      <th>ve</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>windows</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  actually  available  believe  best  better  bit  called  case  com  \\\n",
       "0     0         0          0        0     0       0    0       1     0    0   \n",
       "1     0         0          0        0     0       0    0       0     0    0   \n",
       "2     0         1          0        0     0       1    1       0     0    0   \n",
       "\n",
       "   ...  used  using  ve  want  way  windows  work  world  year  years  \n",
       "0  ...     0      0   0     0    0        0     0      0     0      1  \n",
       "1  ...     0      0   0     0    0        0     0      0     0      0  \n",
       "2  ...     0      0   1     0    1        0     0      0     0      0  \n",
       "\n",
       "[3 rows x 88 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nPrimeras 3 filas del BoW:\")\n",
    "bagofwords.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bf2f8-cf07-4144-819f-791188b021ce",
   "metadata": {},
   "source": [
    "## **Receta 4:** Implementación de TF-IDF\n",
    "\n",
    "TF-IDF es una estadística numérica que mide la relevancia de una palabra en un documento \n",
    "específico dentro de una colección completa de documentos.\n",
    "\n",
    "• **Term Frequency (TF):** Simplemente la cuenta de la palabra en un texto individual.\n",
    "• **Inverse Document Frequency (IDF):** Mide cuán común es la palabra en todos los documentos. \n",
    "  Las palabras que aparecen en casi todos los documentos (como 'the' o 'a') tendrán un bajo peso.\n",
    "\n",
    "TF-IDF pondera la importancia; una palabra es importante si aparece mucho en un texto (tf alto) \n",
    "y pocas veces en el resto de los textos (idf alto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fde2942-6a34-4664-a670-af762a33ace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RECETA 4: TF-IDF\n",
      "======================================================================\n",
      "\n",
      "Dimensiones de la matriz TF-IDF: 11314 filas x 88 columnas\n",
      "\n",
      "Primeras 3 filas del TF-IDF (valores normalizados):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>actually</th>\n",
       "      <th>available</th>\n",
       "      <th>believe</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>bit</th>\n",
       "      <th>called</th>\n",
       "      <th>case</th>\n",
       "      <th>com</th>\n",
       "      <th>...</th>\n",
       "      <th>used</th>\n",
       "      <th>using</th>\n",
       "      <th>ve</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>windows</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165437</td>\n",
       "      <td>0.174116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  actually  available  believe  best    better       bit    called  \\\n",
       "0   0.0  0.000000        0.0      0.0   0.0  0.000000  0.000000  0.412555   \n",
       "1   0.0  0.000000        0.0      0.0   0.0  0.000000  0.000000  0.000000   \n",
       "2   0.0  0.175511        0.0      0.0   0.0  0.165437  0.174116  0.000000   \n",
       "\n",
       "   case  com  ...  used  using        ve  want       way  windows  work  \\\n",
       "0   0.0  0.0  ...   0.0    0.0  0.000000   0.0  0.000000      0.0   0.0   \n",
       "1   0.0  0.0  ...   0.0    0.0  0.000000   0.0  0.000000      0.0   0.0   \n",
       "2   0.0  0.0  ...   0.0    0.0  0.149206   0.0  0.142447      0.0   0.0   \n",
       "\n",
       "   world  year     years  \n",
       "0    0.0   0.0  0.368366  \n",
       "1    0.0   0.0  0.000000  \n",
       "2    0.0   0.0  0.000000  \n",
       "\n",
       "[3 rows x 88 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECETA 4: TF-IDF\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Usar los datos ya limpios de la receta anterior\n",
    "# Configuración de TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=0.05,\n",
    ")\n",
    "\n",
    "# Ajuste y transformación\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_bow[\"text_clean\"])\n",
    "\n",
    "# Captura del TF-IDF en un DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nDimensiones de la matriz TF-IDF: {tfidf_df.shape[0]} filas x {tfidf_df.shape[1]} columnas\"\n",
    ")\n",
    "\n",
    "print(\"\\nPrimeras 3 filas del TF-IDF (valores normalizados):\")\n",
    "tfidf_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "057600ff-d11b-4349-996f-664c427b4bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparación BoW vs TF-IDF para la primera fila:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoW</th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>available</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>believe</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bit</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>called</th>\n",
       "      <td>1</td>\n",
       "      <td>0.412555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           BoW    TF-IDF\n",
       "able         0  0.000000\n",
       "actually     0  0.000000\n",
       "available    0  0.000000\n",
       "believe      0  0.000000\n",
       "best         0  0.000000\n",
       "better       0  0.000000\n",
       "bit          0  0.000000\n",
       "called       1  0.412555\n",
       "case         0  0.000000\n",
       "com          0  0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparar con BoW\n",
    "print(\"\\nComparación BoW vs TF-IDF para la primera fila:\")\n",
    "comparison = pd.DataFrame(\n",
    "    {\"BoW\": bagofwords.iloc[0].head(10), \"TF-IDF\": tfidf_df.iloc[0].head(10)}\n",
    ")\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b3963-3ff6-4928-a4d8-4aa38f80680e",
   "metadata": {},
   "source": [
    "## **Receta 5:** Limpieza y Stemming de Variables de Texto\n",
    "\n",
    "La limpieza o preprocesamiento del texto es crucial antes de crear características \n",
    "(como BoW o TF-IDF) para estandarizar el contenido y mejorar la precisión del modelo.\n",
    "\n",
    "**Pasos de Preprocesamiento:**\n",
    "1. **Eliminación de Puntuación y Números:** Se eliminan caracteres que no son letras o espacios.\n",
    "2. **Configuración de Caso (Lowercase):** Se establece todo el texto en minúsculas.\n",
    "3. **Eliminación de Stop Words:** Se remueven palabras comunes y funcionales.\n",
    "4. **Stemming:** Se reduce cada palabra a su raíz o base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5345c506-8a72-46ac-9959-d9d6c9a06884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RECETA 5: Limpieza y Stemming Completo\n",
      "======================================================================\n",
      "Cargando dataset 20 Newsgroups (train)...\n",
      "✓ Dataset cargado: 11314 documentos\n",
      "\n",
      "\n",
      "TEXTO ORIGINAL:\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "PASO 1: Eliminación de puntuación\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day  It was a 2 door sports car  looked to be from the late 60s \n",
      "early 70s  It was called a Bricklin  The doors were really small  In addition \n",
      "the front bumper was separate from the rest of the body  This is \n",
      "all I know  If anyone can tellme a model name  engine specs  years\n",
      "of production  where this car is made  history  or whatever info you\n",
      "have on this funky looking car  please e mail \n",
      "\n",
      "PASO 2: Eliminación de números\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day  It was a   door sports car  looked to be from the late  s \n",
      "early  s  It was called a Bricklin  The doors were really small  In addition \n",
      "the front bumper was separate from the rest of the body  This is \n",
      "all I know  If anyone can tellme a model name  engine specs  years\n",
      "of production  where this car is made  history  or whatever info you\n",
      "have on this funky looking car  please e mail \n",
      "\n",
      "PASO 3: Conversión a minúsculas\n",
      "i was wondering if anyone out there could enlighten me on this car i saw\n",
      "the other day  it was a   door sports car  looked to be from the late  s \n",
      "early  s  it was called a bricklin  the doors were really small  in addition \n",
      "the front bumper was separate from the rest of the body  this is \n",
      "all i know  if anyone can tellme a model name  engine specs  years\n",
      "of production  where this car is made  history  or whatever info you\n",
      "have on this funky looking car  please e mail \n",
      "\n",
      "PASO 4: Normalización de espacios\n",
      "i was wondering if anyone out there could enlighten me on this car i saw the other day it was a door sports car looked to be from the late s early s it was called a bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all i know if anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please e mail\n",
      "\n",
      "PASO 5: Eliminación de Stop Words\n",
      "wondering anyone could enlighten car saw day door sports car looked late early called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please e mail\n",
      "\n",
      "PASO 6: Stemming (reducción a raíz)\n",
      "wonder anyon could enlighten car saw day door sport car look late earli call bricklin door realli small addit front bumper separ rest bodi know anyon tellm model name engin spec year product car made histori whatev info funki look car pleas e mail\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "COMPARACIÓN ANTES/DESPUÉS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "✗ ANTES (91 palabras):\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "✓ DESPUÉS (44 palabras):\n",
      "wonder anyon could enlighten car saw day door sport car look late earli call bricklin door realli small addit front bumper separ rest bodi know anyon tellm model name engin spec year product car made histori whatev info funki look car pleas e mail\n",
      "\n",
      "======================================================================\n",
      "APLICANDO PIPELINE COMPLETO AL DATASET\n",
      "======================================================================\n",
      "\n",
      "✓ 11314 documentos procesados exitosamente\n",
      "✓ Todos los textos están limpios, sin stop words y con stemming aplicado\n",
      "✓ Datos listos para feature extraction (BoW, TF-IDF, etc.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECETA 5: Limpieza y Stemming Completo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Recargar datos\n",
    "df_clean = load_newsgroups_data(subset=\"train\")\n",
    "\n",
    "# Seleccionar un texto de ejemplo (el primero del dataset)\n",
    "ejemplo_original = df_clean.iloc[0][\"text\"]\n",
    "print(\"\\nTEXTO ORIGINAL:\")\n",
    "print(f\"{ejemplo_original}\\n\")\n",
    "\n",
    "# Paso 1: Eliminación de puntuación\n",
    "ejemplo_paso1 = ejemplo_original\n",
    "ejemplo_paso1 = ejemplo_paso1.replace(r\"[^\\w\\s]\", \" \")\n",
    "# Usar re.sub para que funcione correctamente\n",
    "import re\n",
    "ejemplo_paso1 = re.sub(r\"[^\\w\\s]\", \" \", ejemplo_original)\n",
    "\n",
    "print(\"PASO 1: Eliminación de puntuación\")\n",
    "print(f\"{ejemplo_paso1}\\n\")\n",
    "\n",
    "# Paso 2: Eliminación de números\n",
    "ejemplo_paso2 = re.sub(r\"\\d+\", \" \", ejemplo_paso1)\n",
    "print(\"PASO 2: Eliminación de números\")\n",
    "print(f\"{ejemplo_paso2}\\n\")\n",
    "\n",
    "# Paso 3: Conversión a minúsculas\n",
    "ejemplo_paso3 = ejemplo_paso2.lower()\n",
    "print(\"PASO 3: Conversión a minúsculas\")\n",
    "print(f\"{ejemplo_paso3}\\n\")\n",
    "\n",
    "# Paso 4: Normalizar espacios múltiples\n",
    "ejemplo_paso4 = re.sub(r\"\\s+\", \" \", ejemplo_paso3).strip()\n",
    "print(\"PASO 4: Normalización de espacios\")\n",
    "print(f\"{ejemplo_paso4}\\n\")\n",
    "\n",
    "# Paso 5: Eliminación de Stop Words\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Elimina stop words de manera eficiente.\"\"\"\n",
    "    if pd.isna(text) or not text.strip():\n",
    "        return \"\"\n",
    "    words = [word for word in text.split() if word not in STOP_WORDS]\n",
    "    return \" \".join(words)\n",
    "\n",
    "ejemplo_paso5 = remove_stopwords(ejemplo_paso4)\n",
    "print(\"PASO 5: Eliminación de Stop Words\")\n",
    "print(f\"{ejemplo_paso5}\\n\")\n",
    "\n",
    "# Paso 6: Stemming\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_words(text):\n",
    "    \"\"\"Aplica stemming a cada palabra.\"\"\"\n",
    "    if pd.isna(text) or not text.strip():\n",
    "        return \"\"\n",
    "    words = [STEMMER.stem(word) for word in text.split()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "ejemplo_paso6 = stem_words(ejemplo_paso5)\n",
    "print(\"PASO 6: Stemming (reducción a raíz)\")\n",
    "print(f\"{ejemplo_paso6}\\n\")\n",
    "\n",
    "# Comparación visual final\n",
    "print(\"-\" * 70)\n",
    "print(\"COMPARACIÓN ANTES/DESPUÉS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\n✗ ANTES ({len(ejemplo_original.split())} palabras):\")\n",
    "print(f\"{ejemplo_original}\\n\")\n",
    "print(f\"✓ DESPUÉS ({len(ejemplo_paso6.split())} palabras):\")\n",
    "print(f\"{ejemplo_paso6}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESAMIENTO COMPLETO DEL DATASET\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"APLICANDO PIPELINE COMPLETO AL DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Paso 1: Eliminación de puntuación (reemplazar con espacio)\n",
    "df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"[^\\w\\s]\", \" \", regex=True)\n",
    "\n",
    "# Paso 2: Eliminación de números\n",
    "df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\d+\", \" \", regex=True)\n",
    "\n",
    "# Paso 3: Conversión a minúsculas\n",
    "df_clean[\"text\"] = df_clean[\"text\"].str.lower()\n",
    "\n",
    "# Paso 4: Normalizar espacios múltiples\n",
    "df_clean[\"text\"] = df_clean[\"text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# Paso 5: Eliminación de Stop Words\n",
    "df_clean[\"text\"] = df_clean[\"text\"].apply(remove_stopwords)\n",
    "\n",
    "# Paso 6: Stemming\n",
    "df_clean[\"text\"] = df_clean[\"text\"].apply(stem_words)\n",
    "\n",
    "print(f\"\\n✓ {len(df_clean)} documentos procesados exitosamente\")\n",
    "print(f\"✓ Todos los textos están limpios, sin stop words y con stemming aplicado\")\n",
    "print(f\"✓ Datos listos para feature extraction (BoW, TF-IDF, etc.)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6d88d-c919-41b9-85bb-62fcb7edd110",
   "metadata": {},
   "source": [
    "# Resumen del Pipeline Completo\n",
    "\n",
    "El proceso de feature engineering en texto es similar a un chef que prepara ingredientes \n",
    "para un plato complejo. Inicialmente, tenemos el texto crudo (ingredientes sin procesar). \n",
    "La limpieza (Receta 5) es como pelar y cortar los vegetales (eliminar puntuación, stop words \n",
    "y encontrar la raíz de la palabra) para que sean útiles. Luego, Recetas 1 y 2 miden la cantidad \n",
    "general y el tamaño (¿Cuántos ingredientes hay? ¿Cuántas porciones?). Finalmente, BoW y TF-IDF \n",
    "(Recetas 3 y 4) son como catalogar y ponderar la importancia de cada ingrediente: BoW cuenta \n",
    "cuántas veces se usa el ajo (frecuencia simple), mientras que TF-IDF determina qué tan esencial \n",
    "es el azafrán (un ingrediente raro y específico) en esta receta particular en comparación con \n",
    "todas las demás recetas en el libro (la colección de documentos)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-notebooks)",
   "language": "python",
   "name": "jupyter-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
